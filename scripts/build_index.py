#!/usr/bin/env python3
import json, time, re, shutil
from pathlib import Path
import yaml
from urllib.parse import urlparse

ROOT = Path(__file__).resolve().parents[1]
AUTHORS = ROOT / "authors"
DOCS = ROOT / "docs"
INDEX = ROOT / "index.json"

# æ”¹æˆä½ çš„ä»“åº“åœ°å€
GITHUB_BASE = "https://github.com/wuwuzhijing/awesomeOriginLiterature/blob/main"

def slugify(s: str) -> str:
    """ç”Ÿæˆé€‚åˆæ–‡ä»¶åçš„ slug"""
    return re.sub(r"\W+", "-", s.strip())

# è¯»å–ä½œè€… YAMLï¼Œå¹¶æŒ‰ slug åŽ»é‡
raw_entries = []
for f in sorted(AUTHORS.rglob("*.yaml")):
    try:
        data = yaml.safe_load(f.read_text(encoding="utf-8"))
        if not isinstance(data, dict) or "name" not in data:
            print(f"âš ï¸ è·³è¿‡ {f}ï¼ˆç¼ºå°‘ name å­—æ®µæˆ–æ ¼å¼å¼‚å¸¸ï¼‰")
            continue
        raw_entries.append({"file": f, "data": data})
    except Exception as e:
        print(f"âŒ è§£æžå¤±è´¥ {f}: {e}")

by_slug = {}
duplicates = []
for entry in raw_entries:
    name = entry["data"]["name"]
    slug = slugify(name)
    if slug in by_slug:
        duplicates.append((name, by_slug[slug]["file"], entry["file"]))
    else:
        by_slug[slug] = entry

if duplicates:
    print("\nâš ï¸ å‘çŽ°ä½œè€…é‡åï¼ˆå°†åªä¿ç•™ç¬¬ä¸€ä»½ï¼ŒåŒåçš„å…¶ä½™å¿½ç•¥ï¼‰ï¼š")
    for name, first_f, dup_f in duplicates:
        print(f"   - {name}: {first_f}ï¼ˆä¿ç•™ï¼‰ | {dup_f}ï¼ˆå¿½ç•¥ï¼‰")
    print("")

# åŽ»é‡åŽçš„ä½œè€…ä¸Žæ–‡ä»¶
author_files = [v["file"] for v in by_slug.values()]
authors = [v["data"] for v in by_slug.values()]

# è¾“å‡º index.json
INDEX.write_text(
    json.dumps({"authors": authors, "generated_at": int(time.time())},
               ensure_ascii=False, indent=2),
    encoding="utf-8"
)

# docs ç›®å½•
(DOCS / "authors").mkdir(parents=True, exist_ok=True)

# é¦–é¡µ
with open(DOCS / "index.md", "w", encoding="utf-8") as f:
    f.write("# Literature Archive\n\n")
    f.write(f"å…± {len(authors)} ä½ä½œè€…ã€‚\n\n")
    for a in sorted(authors, key=lambda x: x["name"]):
        f.write(f"- [{a['name']}](authors/{slugify(a['name'])}.md)\n")

# Authors ç´¢å¼•é¡µ
with open(DOCS / "authors" / "index.md", "w", encoding="utf-8") as f:
    f.write("# Authors\n\n")
    for a in sorted(authors, key=lambda x: x["name"]):
        f.write(f"- [{a['name']}](./{slugify(a['name'])}.md)\n")

# é€ä¸ªä½œè€…ç”Ÿæˆé¡µé¢
for f, a in zip(author_files, authors):
    lines = []
    slug = slugify(a["name"])

    # æ ‡é¢˜ & GitHub å…¥å£
    lines.append(f"# {a['name']}")
    rel_path = f.relative_to(ROOT).as_posix()
    github_url = f"{GITHUB_BASE}/{rel_path}"
    lines.append(f"[ðŸ”— åœ¨ GitHub ä¸ŠæŸ¥çœ‹/ç¼–è¾‘è¯¥ä½œè€…æ–‡ä»¶]({github_url})\n")

    # æ ‡ç­¾
    if a.get("tags"):
        lines.append(f"**Tags:** {', '.join(a['tags'])}\n")

    # å¹´è¡¨ï¼šé€šé…æœç´¢ *_timeline.md å¹¶å¤åˆ¶åˆ° docs/authors/
    matches = list(AUTHORS.rglob(f"{slug}_timeline.md"))
    if matches:
        timeline_src = matches[0]
        timeline_dst = DOCS / "authors" / f"{slug}_timeline.md"
        shutil.copyfile(timeline_src, timeline_dst)
        lines.append(f"[ðŸ“œ {a['name']} å¹´è¡¨]({slug}_timeline.md)\n")

    # æ–‡çŒ®æ¸²æŸ“
    originals = [w for w in a.get("works", []) if w.get("type") == "original"]
    secondary = [w for w in a.get("works", []) if w.get("type") == "secondary"]

    def render_block(title, works):
        if not works:
            return
        lines.append(f"## {title}")
        for idx, w in enumerate(works, 1):
            year = w.get("year", "")
            note = w.get("note", "")
            wt = w.get("title", {})

            if isinstance(wt, dict):
                original = wt.get("original", "Untitled")
                en = wt.get("en")
                zh = wt.get("zh")
            else:
                original = str(wt)
                en = zh = None

            lines.append(f"{idx}. **({year}) {original}**  ")
            if en:
                lines.append(f"   *English:* {en}  ")
            if zh:
                lines.append(f"   *ä¸­æ–‡:* {zh}  ")

            links = w.get("links", [])
            if isinstance(links, str):
                links = [links]
            for link in links:
                host = (urlparse(link).hostname or "Link").replace("www.", "")
                lines.append(f"   ðŸ”— [{host}]({link})  ")

            if note:
                lines.append(f"   > {note}")
            lines.append("")

    render_block("Original Literature", originals)
    render_block("Secondary Literature", secondary)

    # èµ„æºæ¸²æŸ“
    resources = a.get("resources", [])
    if resources:
        lines.append("## Resources")
        for idx, r in enumerate(resources, 1):
            title = r.get("title", "Untitled")
            note = r.get("note", "")
            links = r.get("links", [])
            if isinstance(links, str):
                links = [links]

            lines.append(f"{idx}. {title}")
            for link in links:
                host = (urlparse(link).hostname or "Link").replace("www.", "")
                lines.append(f"   ðŸ”— [{host}]({link})  ")
            if note:
                lines.append(f"   > {note}")
            lines.append("")

    content = "\n".join(lines).replace("\\n", "\n")
    (DOCS / "authors" / f"{slug}.md").write_text(content, encoding="utf-8")

print(f"Built docs for {len(authors)} authors.")